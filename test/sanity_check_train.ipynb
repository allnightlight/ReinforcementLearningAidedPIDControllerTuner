{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../framework\"))\n",
    "sys.path.append(os.path.abspath(\"../concrete\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConcAction import ConcAction\n",
    "from ConcObservation import ConcObservation\n",
    "from ConcAgentFactory import ConcAgentFactory\n",
    "from ConcBuildOrder import ConcBuildOrder\n",
    "from ConcEnvrionmentFactory import ConcEnvironmentFactory\n",
    "from ConcEnvironment import ConcEnvironment\n",
    "from ConcRewardGiverFactory import ConcRewardGiverFactory\n",
    "from ConcTrainerFactory import ConcTrainerFactory\n",
    "from ConcValueFunctionApproximatorFactory import ConcValueFunctionApproximatorFactory\n",
    "from framework import Trainer, ObservationSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the history over the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveYURfromTrainer(trainer):\n",
    "\n",
    "    Y = []\n",
    "    for observationSequence in trainer.historyObservationSequences:\n",
    "        y = observationSequence[-1].getValue() # (1, nPv)\n",
    "        Y.append(y)\n",
    "    Y = np.concatenate(Y, axis=0) # (*, nPv)\n",
    "\n",
    "    U = []\n",
    "    for action in trainer.historyActions:\n",
    "        u = action.getValue() # (1, nMv)\n",
    "        U.append(u)\n",
    "    U = np.concatenate(U, axis=0) # (*, nMv)\n",
    "\n",
    "    R = []\n",
    "    for reward in trainer.historyRewards:\n",
    "        r = reward.getValue() # (1,)\n",
    "        R.append(r)\n",
    "    R = np.concatenate(R, axis=0) # (*,)\n",
    "    \n",
    "    return Y, U, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildOrder = ConcBuildOrder(nIteration=100\n",
    "                            , nSeq=1\n",
    "                            , nHorizonValueOptimization=100\n",
    "                            , nIntervalPolicyOptimization=100\n",
    "                            , nBatchPolicyOptimization=2**5\n",
    "                            , nSaveInterval=2**5\n",
    "                            , description=\"test\"\n",
    "                            , tConstant = 10\n",
    "                            , nHiddenValueApproximator = 2**5\n",
    "                            , sdPolicy = 0.0\n",
    "                            , nActionsSampledFromPolicy = 2**0\n",
    "                            , amplitudeDv = 0.0\n",
    "                            )\n",
    "\n",
    "agent = ConcAgentFactory().create(buildOrder)\n",
    "environment = ConcEnvironmentFactory().create(buildOrder)\n",
    "valueFunctionApproximator = ConcValueFunctionApproximatorFactory().create(buildOrder)\n",
    "rewardGiver = ConcRewardGiverFactory().create(buildOrder)\n",
    "\n",
    "trainerFactory = ConcTrainerFactory()\n",
    "trainer = trainerFactory.create(agent, environment, valueFunctionApproximator, rewardGiver, buildOrder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.init()\n",
    "trainer.train(1)\n",
    "agent.gainP.weights[0].assign(np.zeros((1,ConcEnvironment.nMv)))\n",
    "agent.gainP.weights[1].assign(np.zeros((ConcEnvironment.nMv,)))\n",
    "trainer.train(2**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, U, R = retrieveYURfromTrainer(trainer)\n",
    "\n",
    "fig = plt.figure()\n",
    "for k1, x in enumerate((Y, U, R)):\n",
    "    ax = fig.add_subplot(3,1,k1+1)\n",
    "    ax.plot(x)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the history of ValueFunction without policy update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nitr = 2**12\n",
    "\n",
    "buildOrder = ConcBuildOrder(nIteration=Nitr\n",
    "                            , nSeq=1\n",
    "                            , nHorizonValueOptimization=2**3\n",
    "                            , nIntervalPolicyOptimization=Nitr\n",
    "                            , nBatchPolicyOptimization=2**5\n",
    "                            , nSaveInterval=2**5\n",
    "                            , description=\"test\"\n",
    "                            , tConstant = 10\n",
    "                            , nHiddenValueApproximator = 2**3\n",
    "                            , sdPolicy = 1.0\n",
    "                            , nActionsSampledFromPolicy = 2**0                      \n",
    "                            )\n",
    "\n",
    "agent = ConcAgentFactory().create(buildOrder)\n",
    "environment = ConcEnvironmentFactory().create(buildOrder)\n",
    "valueFunctionApproximator = ConcValueFunctionApproximatorFactory().create(buildOrder)\n",
    "rewardGiver = ConcRewardGiverFactory().create(buildOrder)\n",
    "\n",
    "trainerFactory = ConcTrainerFactory()\n",
    "trainer = trainerFactory.create(agent, environment, valueFunctionApproximator, rewardGiver, buildOrder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.init()\n",
    "trainer.train(1)\n",
    "\n",
    "agent.gainP.weights[0].assign(np.zeros((1,ConcEnvironment.nMv)))\n",
    "agent.gainP.weights[1].assign(np.zeros((ConcEnvironment.nMv,)))\n",
    "\n",
    "trainer.train(buildOrder.nIteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 =  np.zeros((1, ConcEnvironment.nMv)) # (1, nMv)\n",
    "u1p =  0.5 * np.ones((1, ConcEnvironment.nMv)) # (1, nMv)\n",
    "u1n =  -0.5 * np.ones((1, ConcEnvironment.nMv)) # (1, nMv)\n",
    "\n",
    "y0 = np.zeros((1, ConcEnvironment.nPv)).astype(np.float32) # (1, nPv)\n",
    "y1p = 0.5 * np.ones((1, ConcEnvironment.nPv)).astype(np.float32) # (1, nPv)\n",
    "y1n = -0.5 * np.ones((1, ConcEnvironment.nPv)).astype(np.float32) # (1, nPv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under u = 0, compare the value with y = 0, +1, -1, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = ConcAction(u0)\n",
    "observation = ConcObservation(y0)\n",
    "observationSequence = ObservationSequence()\n",
    "observationSequence.add(observation)\n",
    "\n",
    "value = valueFunctionApproximator(observationSequence, action)\n",
    "_aValue, _sValue = value.getValue() # (1, 1)\n",
    "print(_aValue, _sValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = ConcAction(u0)\n",
    "observation = ConcObservation(y1p)\n",
    "observationSequence = ObservationSequence()\n",
    "observationSequence.add(observation)\n",
    "\n",
    "value = valueFunctionApproximator(observationSequence, action)\n",
    "_aValue, _sValue = value.getValue() # (1, 1)\n",
    "print(_aValue, _sValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = ConcAction(u0)\n",
    "observation = ConcObservation(y1n)\n",
    "observationSequence = ObservationSequence()\n",
    "observationSequence.add(observation)\n",
    "\n",
    "value = valueFunctionApproximator(observationSequence, action)\n",
    "_aValue, _sValue = value.getValue() # (1, 1)\n",
    "print(_aValue, _sValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the history of policy update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nIntervalPolicyOptimization = 2**3\n",
    "buildOrder = ConcBuildOrder(nIteration=2**10\n",
    "                            , nSeq=1\n",
    "                            , nHorizonValueOptimization=nIntervalPolicyOptimization//2\n",
    "                            , nIntervalPolicyOptimization=nIntervalPolicyOptimization\n",
    "                            , nBatchPolicyOptimization=2**5\n",
    "                            , nSaveInterval=2**5\n",
    "                            , description=\"test\"\n",
    "                            , tConstant = 10\n",
    "                            , nHiddenValueApproximator = 2**3\n",
    "                            , sdPolicy = 0.1\n",
    "                            , nActionsSampledFromPolicy = 2**0\n",
    "                            )\n",
    "\n",
    "agent = ConcAgentFactory().create(buildOrder)\n",
    "environment = ConcEnvironmentFactory().create(buildOrder)\n",
    "valueFunctionApproximator = ConcValueFunctionApproximatorFactory().create(buildOrder)\n",
    "rewardGiver = ConcRewardGiverFactory().create(buildOrder)\n",
    "\n",
    "trainerFactory = ConcTrainerFactory()\n",
    "trainer = trainerFactory.create(agent, environment, valueFunctionApproximator, rewardGiver, buildOrder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.init()\n",
    "trainer.train(1)\n",
    "\n",
    "agent.gainP.weights[0].assign(np.zeros((1,ConcEnvironment.nMv)))\n",
    "agent.gainP.weights[1].assign(np.zeros((ConcEnvironment.nMv,)))\n",
    "\n",
    "Gain = []\n",
    "Bias = []\n",
    "for k1 in range(2**10):\n",
    "    sys.stdout.write('\\r%04d' % k1)\n",
    "    gain = agent.gainP.weights[0].numpy() # (1, nMv)\n",
    "    bias = agent.gainP.weights[1].numpy() # (nMv, )\n",
    "    Gain.append(gain)\n",
    "    Bias.append(bias)\n",
    "    trainer.train(nIntervalPolicyOptimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gain = np.concatenate(Gain, axis=0) # (*, nMv)\n",
    "Bias = np.stack(Bias, axis=0) # (*, nMv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(Gain)\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(Bias)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
