{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../framework\"))\n",
    "sys.path.append(os.path.abspath(\"../concrete\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AsmAction import AsmAction\n",
    "from AsmSimulator import AsmSimulator\n",
    "from ConcObservation import ConcObservation\n",
    "from ConcAgentFactory import ConcAgentFactory\n",
    "from ConcBuildOrder import ConcBuildOrder\n",
    "from ConcEnvrionmentFactory import ConcEnvironmentFactory\n",
    "from ConcRewardGiverFactory import ConcRewardGiverFactory\n",
    "from ConcTrainerFactory import ConcTrainerFactory\n",
    "from ConcValueFunctionApproximatorFactory import ConcValueFunctionApproximatorFactory\n",
    "from framework import Trainer, ObservationSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S100: Check the responce of Asm Simulator to the actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S110: steady state with various set values of DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkSteadyState(action):\n",
    "    # ignore the fluctuation of inflow\n",
    "    asmSimulator =  AsmSimulator(h = 1.0)\n",
    "    asmSimulator.init()\n",
    "\n",
    "    Y = []\n",
    "    X = []\n",
    "\n",
    "    for _ in range(14): # 1[step] = [1day]\n",
    "        asmSimulator.update(action)\n",
    "        observation = asmSimulator.observe()\n",
    "        Y.append(observation.getValue())\n",
    "        X.append(asmSimulator.x)\n",
    "\n",
    "    Ynumpy = np.concatenate(Y, axis=0) # (*, nPv = 1)\n",
    "    Xnumpy = np.stack(X, axis=0) # (*, nAsm)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(Ynumpy)\n",
    "    plt.title('NH4')\n",
    "\n",
    "    for k1, name in enumerate(asmSimulator.asmVarNames):\n",
    "        print(\"{0:5s} {1:8.2f}\".format(name, asmSimulator.x[k1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DO = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = AsmAction(np.ones((1,1)) * 10.0) # (1, nMv = 1)\n",
    "Do = action.getActionOnEnvironment()\n",
    "checkSteadyState(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DO = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = AsmAction(np.ones((1,1)) * -1.0) # (1, nMv = 1)\n",
    "Do = action.getActionOnEnvironment()\n",
    "checkSteadyState(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DO = 0.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = AsmAction(np.ones((1,1)) * -3.0) # (1, nMv = 1)\n",
    "Do = action.getActionOnEnvironment()\n",
    "checkSteadyState(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S120: steady state with cyclic pertubated inflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkSteadyStateWithPertubation(action):\n",
    "    # ignore the fluctuation of inflow\n",
    "    asmSimulator =  AsmSimulator(amplitudePeriodicDv=2.0)\n",
    "    asmSimulator.init()\n",
    "\n",
    "    Y = []\n",
    "    X = []\n",
    "\n",
    "    for _ in range(96*3): # 96[step] = [1day]\n",
    "        asmSimulator.update(action)\n",
    "        observation = asmSimulator.observe()\n",
    "        Y.append(observation.S_NH4)\n",
    "        X.append(asmSimulator.x)\n",
    "\n",
    "    Ynumpy = np.stack(Y, axis=0) # (*, nPv = 1)\n",
    "    Xnumpy = np.stack(X, axis=0) # (*, nAsm)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(Ynumpy)\n",
    "    plt.title('NH4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DO = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = AsmAction(np.ones((1,1)) * 10.0) # (1, nMv = 1)\n",
    "Do = action.getActionOnEnvironment()\n",
    "checkSteadyStateWithPertubation(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DO = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = AsmAction(np.ones((1,1)) * -1.0) # (1, nMv = 1)\n",
    "Do = action.getActionOnEnvironment()\n",
    "checkSteadyStateWithPertubation(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DO = 0.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = AsmAction(np.ones((1,1)) * -2.0) # (1, nMv = 1)\n",
    "Do = action.getActionOnEnvironment()\n",
    "checkSteadyStateWithPertubation(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S130: closed loop simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runClosedLoopSimulation(nSteps):\n",
    "    # ignore the fluctuation of inflow\n",
    "    asmSimulator =  AsmSimulator(amplitudePeriodicDv=2.0, pgain=100, timeIntegral=1/24/4)\n",
    "    asmSimulator.init()\n",
    "\n",
    "    NH4 = []\n",
    "    Do = []\n",
    "    O2 = []\n",
    "\n",
    "    gain = 2\n",
    "    bias = -3.2\n",
    "    \n",
    "    for _ in range(nSteps): # 96[step] = [1day]\n",
    "        \n",
    "        observation = asmSimulator.observe()        \n",
    "        u = observation.getValue() * gain + bias\n",
    "        \n",
    "        action = AsmAction(u)\n",
    "        \n",
    "        asmSimulator.update(action)\n",
    "        observation = asmSimulator.observe()\n",
    "        NH4.append(observation.S_NH4)\n",
    "        Do.append(float(action.getActionOnEnvironment()))\n",
    "        O2.append(asmSimulator.x[asmSimulator.idxSO2])\n",
    "\n",
    "    NH4 = np.stack(NH4, axis=0) # (*, )\n",
    "    Do = np.stack(Do, axis=0) #  (*, )\n",
    "    O2 = np.stack(O2, axis=0) #  (*, )\n",
    "    \n",
    "    return NH4, Do, O2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NH4, Do, O2 = runClosedLoopSimulation(96*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(Do, label = 'DO')\n",
    "plt.plot(O2, label = 'S_O2')\n",
    "plt.legend()\n",
    "#\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(NH4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S200. Check the history of policy update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveYURfromTrainer(trainer):\n",
    "\n",
    "    Y = []\n",
    "    for observationSequence in trainer.historyObservationSequences:\n",
    "        y = observationSequence[-1].getValue() # (1, nPv)\n",
    "        Y.append(y)\n",
    "    Y = np.concatenate(Y, axis=0) # (*, nPv)\n",
    "\n",
    "    U = []\n",
    "    for action in trainer.historyActions:\n",
    "        u = action.getActionOnEnvironment() # (1, nMv)\n",
    "        U.append(u)\n",
    "    U = np.concatenate(U, axis=0) # (*, nMv)\n",
    "\n",
    "    R = []\n",
    "    for reward in trainer.historyRewards:\n",
    "        r = reward.getValue() # (1,)\n",
    "        R.append(r)\n",
    "    R = np.concatenate(R, axis=0) # (*,)\n",
    "    \n",
    "    return Y, U, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportTraceOfTraining(trainer, Gain, Bias):\n",
    "    Y, U, R = retrieveYURfromTrainer(trainer)\n",
    "    \n",
    "    GainNumpy = np.concatenate(Gain, axis=0) # (*, nMv)\n",
    "    BiasNumpy = np.stack(Bias, axis=0) # (*, nMv)\n",
    "\n",
    "    fig = plt.figure(figsize=[25/2.57, 12/2.57])\n",
    "    #\n",
    "    plt.subplot(3,1,1)\n",
    "    plt.plot(Y)\n",
    "    plt.ylabel('NH4')\n",
    "    #\n",
    "    plt.subplot(3,1,2)\n",
    "    plt.plot(U)\n",
    "    plt.ylabel('DO')\n",
    "    #\n",
    "    plt.subplot(3,1,3)\n",
    "    plt.plot(R)\n",
    "    plt.ylabel('Reward')\n",
    "    #\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('trace.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    plt.figure(figsize=[25/2.57, 12/2.57])    \n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(GainNumpy)\n",
    "    plt.ylabel('Gain')\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(BiasNumpy)\n",
    "    plt.ylabel('Bias')    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('parameter trace.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSS210: train an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nIntervalPolicyOptimization = 2**1\n",
    "buildOrder = ConcBuildOrder(nIteration=2**10\n",
    "                            , nSeq=1\n",
    "                            , nHorizonValueOptimization=nIntervalPolicyOptimization\n",
    "                            , nIntervalPolicyOptimization=nIntervalPolicyOptimization\n",
    "                            , nBatchPolicyOptimization=2**5\n",
    "                            , nSaveInterval=2**5\n",
    "                            , description=\"test\"\n",
    "#                             , tConstant = 10\n",
    "                            , nHiddenValueApproximator = 2**3\n",
    "                            , sdPolicy = 0.1\n",
    "                            , nActionsSampledFromPolicy = 2**0                            \n",
    "#                             , amplitudeDv = 0.0\n",
    "                            , amplitudePeriodicDv = 2.0\n",
    "                            , agentUseBias = True\n",
    "                            , learningRateValueFunctionOptimizer = 1e-2\n",
    "                            , weightOnError = 0.9\n",
    "                            , policyOptimizer = \"Adam\"\n",
    "                            , valueFunctionOptimizer = \"Adam\"\n",
    "                            , returnType = \"SumOfInfiniteRewardSeries\"\n",
    "                            , gamma = 0.9\n",
    "                            , environmentName = \"AsmSimulator\"\n",
    "                            )\n",
    "\n",
    "agent = ConcAgentFactory().create(buildOrder)\n",
    "environment = ConcEnvironmentFactory().create(buildOrder)\n",
    "valueFunctionApproximator = ConcValueFunctionApproximatorFactory().create(buildOrder)\n",
    "rewardGiver = ConcRewardGiverFactory().create(buildOrder)\n",
    "\n",
    "trainerFactory = ConcTrainerFactory()\n",
    "trainer = trainerFactory.create(agent, environment, valueFunctionApproximator, rewardGiver, buildOrder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.init()\n",
    "trainer.train(1)\n",
    "\n",
    "agent.gainP.weights[0].assign(np.zeros((1,environment.nMv)))\n",
    "agent.gainP.weights[1].assign(np.zeros((environment.nMv,)))\n",
    "\n",
    "Gain = []\n",
    "Bias = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_bgn = datetime.now()\n",
    "for k1 in range(2**16):\n",
    "    sys.stdout.write('\\r%s  %04d' % (datetime.now() - t_bgn, k1))\n",
    "    gain = agent.gainP.weights[0].numpy() # (1, nMv)\n",
    "    bias = agent.gainP.weights[1].numpy() # (nMv, )\n",
    "    Gain.append(gain)\n",
    "    Bias.append(bias)\n",
    "    trainer.train(nIntervalPolicyOptimization)\n",
    "    \n",
    "    if k1 % 2**8 == 0:\n",
    "        exportTraceOfTraining(trainer, Gain, Bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
